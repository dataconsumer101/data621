---
title: 'HW2: Classification'
author: "Critical Thinking Group One"
date: "`r Sys.Date()`" # Due 3/21/2021
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: no
    theme: cerulean
    highlight: tango
    font-family: Arial
  pdf_document:
    toc: yes
---

<style type="text/css">

code.r {
  font-family: "Consolas", monospace;
  font-size: 11px;
}

pre {
  font-family: "Consolas", monospace;
  font-size: 11px;
}

</style>


Hey y'all, this is the Pima Indian Diabetes dataset.
There's lots things floating around on the internets with this set.
[https://www.kaggle.com/uciml/pima-indians-diabetes-database](https://www.kaggle.com/uciml/pima-indians-diabetes-database)


## Authorship

**Critical Thinking Group 1** 

- Angel Claudio
- Bonnie Cooper
- Manolis Manoli
- Magnus Skonberg
- Christian Thieme
- Leo Yi

```{r setup, include=FALSE}
# A.C. Echo set to auto false since professor interested in results only.
# L.Y. setting Echo to T b/c some of the questions are to write functions --
knitr::opts_chunk$set(echo = T, warning = F, eval = T, class.source = "tinyfonts", class.output = 'tinyfonts') 

library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
library( caret )
library( pROC )


options(scipen = 9)
set.seed(123)
```

```{r message=FALSE, warning=FALSE}
classification_df <- readr::read_csv('https://raw.githubusercontent.com/dataconsumer101/data621/main/hw2/classification-output-data.csv')

# a bit easier to work with --
df <- select(classification_df, class, scored.class, scored.probability)
```

```{r, class.output = 'tinyfonts' }
glimpse( classification_df )
```
*I'm assuming that 0 = does not have diabetes and 1 = has diabetes, but can't find anywhere the specifies this.* 

## Exercises

### 2

Use the table() function to get the raw confusion matrix for this scored dataset. 

```{r}
#Use the table() function to get the raw confusion matrix for this scored dataset.
conf <- classification_df %>% 
    dplyr::select( scored.class, class ) %>%
    table()
conf

### Formatted version of confusion matrix -- LY
# Convert to factors in order to reorder into a confusion matrix
df$actual <- factor(df$class, levels = c(1,0), labels = c('Diabetes', 'No Diabetes'))
df$predicted <- factor(df$scored.class, levels = c(1,0), labels = c('Diabetes', 'No Diabetes'))

with(df, table(predicted, actual))
```

The confusion matrix organizes tallies for observations based on the true state of the world into columns and predicted classification across rows. The values in the left column represent the number of cases where the model predicted no diabetes and the case indeed did not have diabetes (top) and the number of cases where the model assigned a positive diabetes diagnosis when this was false (bottom). Conversely, the values in the right column represent the cases where the model predicted no diabetes when in fact the case was positive (top) and the occurrences where the model predicted positive diabetes and the patient indeed had diabetes (bottom).


Lets use this table to create our 4 variables for 

true positive TP
```{r}
TP <- conf[2,2]
TP
```

true negative TN
```{r}
TN <- conf[1,1]
TN
```

false positive FP
```{r}
FP <- conf[2,1]
FP
```

false negative FN
```{r}
FN <- conf[1,2]
FN
```

### 3

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.

The Accuracy is then given by $\frac{TP + TN}{TP + FP + TN + FN}$ 
```{r}
Acc <- (TP + TN)/(TP + FP + TN + FN)
Acc

# Function using dataset
accuracy <- function(df, actual_field, predicted_field) {
  
  TP <- sum(df[actual_field] == 1 & df[predicted_field] == 1)
  TN <- sum(df[actual_field] == 0 & df[predicted_field] == 0)
  FP <- sum(df[actual_field] == 0 & df[predicted_field] == 1)
  FN <- sum(df[actual_field] == 1 & df[predicted_field] == 0)

  Acc <- (TP + TN)/(TP + FP + TN + FN)
  return(Acc)
}

(a <- accuracy(df, actual_field = 'class', predicted_field = 'scored.class'))
```

### 4

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.

The Classification Error Rate is given by $\frac{FP + FN}{TP + FP + TN + FN}$ 
```{r}
ClassErr <- (FP + FN)/(TP + FP + TN + FN)
ClassErr

# Function using dataset
classification_error_rate <- function(df, actual_field, predicted_field) {
  
  TP <- sum(df[actual_field] == 1 & df[predicted_field] == 1)
  TN <- sum(df[actual_field] == 0 & df[predicted_field] == 0)
  FP <- sum(df[actual_field] == 0 & df[predicted_field] == 1)
  FN <- sum(df[actual_field] == 1 & df[predicted_field] == 0)

  ClassErr <- (FP + FN)/(TP + FP + TN + FN)
  return(ClassErr)
}

(cer <- classification_error_rate(df, actual_field = 'class', predicted_field = 'scored.class'))

# Accuracy + Classification Error Rate check
a + cer
```

We can see that .807 + .193 sum to 1 as we would expect

### 5

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.

The Precision is given by $\frac{TP}{TP + FP}$ 
```{r}
Prec <- (TP)/(TP + FP)
Prec

# Function using dataset
precision_function <- function(df, actual_field, predicted_field) {
  
  TP <- sum(df[actual_field] == 1 & df[predicted_field] == 1)
  TN <- sum(df[actual_field] == 0 & df[predicted_field] == 0)
  FP <- sum(df[actual_field] == 0 & df[predicted_field] == 1)
  FN <- sum(df[actual_field] == 1 & df[predicted_field] == 0)

  Prec <- (TP)/(TP + FP)
  return(Prec)
}

precision_function(df, actual_field = 'class', predicted_field = 'scored.class')
```

### 6

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.

The Sensitivity is given by $\frac{TP}{TP + FN}$ 
```{r}
Sens <- (TP)/(TP + FN)
Sens

# Function using dataset
sensitivity_function <- function(df, actual_field, predicted_field) {
  
  TP <- sum(df[actual_field] == 1 & df[predicted_field] == 1)
  TN <- sum(df[actual_field] == 0 & df[predicted_field] == 0)
  FP <- sum(df[actual_field] == 0 & df[predicted_field] == 1)
  FN <- sum(df[actual_field] == 1 & df[predicted_field] == 0)

  Sens <- (TP)/(TP + FN)
  return(Sens)
}

sensitivity_function(df, actual_field = 'class', predicted_field = 'scored.class')
```

### 7

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.

The Specificity is given by $\frac{TN}{TN + FP}$ 
```{r}
Speci <- (TN)/(TN + FP)
Speci

# Function using dataset
specificity_function <- function(df, actual_field, predicted_field) {
  
  TP <- sum(df[actual_field] == 1 & df[predicted_field] == 1)
  TN <- sum(df[actual_field] == 0 & df[predicted_field] == 0)
  FP <- sum(df[actual_field] == 0 & df[predicted_field] == 1)
  FN <- sum(df[actual_field] == 1 & df[predicted_field] == 0)

  Speci <- (TN)/(TN + FP)
  return(Speci)
}

specificity_function(df, actual_field = 'class', predicted_field = 'scored.class')
```

### 8

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions.

```{r echo = T}
F1Score <- function (df) {
conf <- df %>% 
    dplyr::select( scored.class, class ) %>%
    table()
TP <- conf[2,2]
TN <- conf[1,1]
FP <- conf[2,1]
FN <- conf[1,2]
Prec <- (TP)/(TP + FP)
Sens <- (TP)/(TP + FN)
result <- (2 * Prec * Sens)/(Prec + Sens)
return(result)
}

F1Score(classification_df)


# Function using dataset
f1_score <- function(df, actual_field, predicted_field) {
  
  TP <- sum(df[actual_field] == 1 & df[predicted_field] == 1)
  TN <- sum(df[actual_field] == 0 & df[predicted_field] == 0)
  FP <- sum(df[actual_field] == 0 & df[predicted_field] == 1)
  FN <- sum(df[actual_field] == 1 & df[predicted_field] == 0)

  Prec <- (TP)/(TP + FP)
  Sens <- (TP)/(TP + FN)
  f1 <- (2 * Prec * Sens)/(Prec + Sens)
  return(f1)
}

f1_score(df, actual_field = 'class', predicted_field = 'scored.class')
```

### 9

What are the bounds on the F1 score?

Since precision and sensitivity are values between 0 and 1, we can show the upper and lower bounds of F1 where the respective values are .01,.01 and .99,.99:

```{r}
f1_quick <- function(p, s) {
  f1 <- (2 * p * s) / (p + s)
  return(f1)
}

# lower bound
# precision = 0.01
# sensitivity = 0.01
f1_quick(.01, .01)

# upper bound
# precision = 0.99
# sensitivity = 0.99
f1_quick(.99, .99)
```

### 10

Write a function that generates an ROC curve from a data set with a true classification column and probability column.

```{r}

```

### 11

Use your **created R functions** and the provided classification output data set to produce all of the classification metrics discussed above

```{r}
cl_accuracy <- accuracy(df, actual_field = 'class', predicted_field = 'scored.class')
cl_error_rate <- classification_error_rate(df, actual_field = 'class', predicted_field = 'scored.class')
cl_precision <- precision_function(df, actual_field = 'class', predicted_field = 'scored.class')
cl_sensitivity <- sensitivity_function(df, actual_field = 'class', predicted_field = 'scored.class')
cl_specificity <- specificity_function(df, actual_field = 'class', predicted_field = 'scored.class')
cl_f1_score <- f1_score(df, actual_field = 'class', predicted_field = 'scored.class')

data.frame()
```

### 12

Investigate the **caret** package. In particular, consider the functions confusionMatrix, sensitivity, and specificity, Apply the functions to the data set. How do the results compare with your own functions?

```{r}

```

### 13

Investigate the **pROC** package. Use it to generate an ROC curve for the data set. How do the results compare with your own functions?


<br><br>